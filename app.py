import streamlit as st, requests, json, os
from utils.schemas import RawMeasurement
from datetime import datetime

st.set_page_config(page_title="Enerji AI AsistanÄ±", layout="wide")

PRED_ENDPOINT = "http://localhost:8002/predict"

# ----- Sidebar -----
st.sidebar.title("Breaker â Makine EÅŸleme")
st.sidebar.text("SÃ¼rÃ¼kleâ€‘bÄ±rak diyagram ileride gelecekâ€¦")

# ----- Tablar -----
tab_dash, tab_upload, tab_chat = st.tabs(["ğŸ“Š Dashboard", "ğŸ“‚ PDF Upload", "ğŸ¤– Chatbot"])

with tab_dash:
    st.header("CanlÄ± Ã–lÃ§Ã¼mler")
    if "measurements" not in st.session_state:
        st.session_state.measurements = []
    col1, col2 = st.columns(2)
    with col1:
        breaker_id = st.text_input("Breaker ID", value="BRK-12-A1")
        current = st.number_input("AkÄ±m (A)", value=30.0)
        voltage = st.number_input("Gerilim (V)", value=400.0)
        if st.button("GÃ¶nder"):
            measurement = RawMeasurement(
                timestamp=datetime.utcnow(), breaker_id=breaker_id,
                metrics={"current": current, "voltage": voltage,
                         "active_power": current*voltage/1000,
                         "reactive_power": 0, "apparent_power":0,
                         "power_factor":0.9, "energy":0,
                         "leakage_current":0, "temperature":25}
            )
            st.session_state.measurements.append(measurement)
    with col2:
        if st.button("24Â saat Tahmin"):
            js = [m.model_dump_json() for m in st.session_state.measurements]
            resp = requests.post(PRED_ENDPOINT, json={"data": js}).json()
            st.metric("Beklenen Fatura (TL)", resp["expected_cost"])

with tab_upload:
    st.header("Makine PDF YÃ¼kle")
    pdf_file = st.file_uploader("Teknik dÃ¶kÃ¼manÄ± seÃ§", type=["pdf"])
    if pdf_file:
        st.success(f"YÃ¼klendi: {pdf_file.name} (Ã¶rnek embed iÅŸlemi burada yapÄ±lacak)")

with tab_chat:
    st.header("Enerji Chatbot")
    if "messages" not in st.session_state:
        st.session_state.messages = []
    for m in st.session_state.messages:
        st.chat_message(m["role"]).markdown(m["content"])
    if prompt := st.chat_input("Sorunuzu yazÄ±nâ€¦"):
        st.chat_message("user").markdown(prompt)
        st.session_state.messages.append({"role":"user","content":prompt})
        # ----- LLM Ã§aÄŸrÄ±sÄ± -----
        from llm.agent import agent
        answer = agent.invoke({"input": prompt})
        st.chat_message("assistant").markdown(answer["output"])
        st.session_state.messages.append({"role":"assistant","content":answer["output"]})
