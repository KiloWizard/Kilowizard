from langchain_openai import ChatOpenAI
from ml.predict import predict_energy
from ml.predict import  fault_detection
from ml.predict import leakage_anomaly_detection
import re
import json


llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.2,
    openai_api_key=OPENAI_API_KEY
)

# Prompt oluÅŸtur
def build_prompt(data: dict) -> str:
    user_question = data.get("input", "")
    devices = data.get("devices", [])
    predict_result = data.get("predict_result", {})
    faults = data.get("faults", {})
    leakage_result = data.get("leakage_result", {})

    prompt_parts = [
        "Sen bir enerji verimliliÄŸi danÄ±ÅŸmanÄ±sÄ±n. KullanÄ±cÄ±ya aÅŸaÄŸÄ±daki verilere gÃ¶re yanÄ±t ver.\n"
    ]

    # ğŸ”¹ Cihaz bilgileri (varsa)
    if devices:
        for i, dev in enumerate(devices, 1):
            prompt_parts.append(f"\nğŸ”¹ Cihaz {i}")
            prompt_parts.append(f"Cihaz AdÄ±: {dev.get('cihaz_adi', '-')}")
            prompt_parts.append(f"Cihaz ID: {dev.get('Cihaz_id', '-')}")
            prompt_parts.append(f"Breaker ID: {dev.get('breaker_id', '-')}")
            prompt_parts.append(f"YÃ¼klenen PDF: {dev.get('cihaz_pdf', '-')}")

            not_ = dev.get("kullanici_promptu", "")
            if not_:
                prompt_parts.append(f"KullanÄ±cÄ± Notu: {not_}")

            text = dev.get("pdf_text", "")
            if text:
                prompt_parts.append("PDF Teknik Ä°Ã§eriÄŸi:\n" + text[:2000] + "\n---")


    # ğŸ”¸ Tahmini fatura bilgileri (her zaman)
    if predict_result:
        prompt_parts.append("\nğŸ”¸ AI destekli fatura tahmini sonuÃ§larÄ±:")
        prompt_parts.append(f"- Tahmini gÃ¼nlÃ¼k tÃ¼ketim (kWh): {predict_result.get('daily_predictions_kWh', [])[:5]} ...")
        prompt_parts.append(f"- AylÄ±k toplam enerji tÃ¼ketimi (kWh): {predict_result.get('total_energy_kWh', '-')} kWh")
        prompt_parts.append(f"- Tahmini fatura tutarÄ± (TL): {predict_result.get('estimated_cost_TL', '-')} TL")
        prompt_parts.append(
            "\nBu tahmin sonuÃ§larÄ±nÄ± KULLAN ve kullanÄ±cÄ±ya net, kibar ve anlaÅŸÄ±lÄ±r bir cevap ver. "
            "KullanÄ±cÄ± sadece tahmini soruyorsa bunlarÄ± Ã¶zetle. Ekstra bilgi sorarsa aÃ§Ä±klama yap."
        )

    if faults:
        prompt_parts.append("\nâš ï¸ Anomali tespiti:")
        for breaker, dates in faults.items():
            formatted_dates = ', '.join(dates)
            prompt_parts.append(f"- {breaker}: ÅÃ¼pheli gÃ¼nler â†’ {formatted_dates}")
        prompt_parts.append(
            "\nKullanÄ±cÄ± arÄ±za hakkÄ±nda soru sorarsa bu bilgileri detaylÄ± paylaÅŸ. Sormazsa sadece Ã¶zet geÃ§.")

    if leakage_result:
        prompt_parts.append("\nâš¡ KaÃ§ak akÄ±m anomalisi tespiti:")
        for breaker, dates in leakage_result.items():
            formatted_dates = ', '.join(dates)
            prompt_parts.append(f"- {breaker}: ÅÃ¼pheli gÃ¼nler â†’ {formatted_dates}")
        prompt_parts.append(
            "\nKullanÄ±cÄ± kaÃ§ak akÄ±m hakkÄ±nda soru sorarsa bu bilgileri detaylÄ± paylaÅŸ. Sormazsa sadece Ã¶zet geÃ§."
        )

    prompt_parts.append(f"\nKullanÄ±cÄ±nÄ±n Sorusu: {user_question}")
    return "\n".join(prompt_parts)

    # KullanÄ±cÄ± sorusu
    prompt_parts.append(f"\nKullanÄ±cÄ±nÄ±n Sorusu: {user_question}")
    return "\n".join(prompt_parts)

# LLM cevabÄ±nÄ± dÃ¼zelt
def fix_output(text: str) -> str:
    text = re.sub(r"(\d)\.(\S)", r"\1. \2", text)
    text = re.sub(r"(\d\.\s)([A-ZÃ‡ÄÄ°Ã–ÅÃœ])", r"\n\n\1\2", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    text = re.sub(r"[ ]{2,}", " ", text)

    lines = text.splitlines()
    fixed_lines = []
    for line in lines:
        line = line.strip()
        if line and not line.endswith((".", ":", "-", "â€¦")):
            if not re.match(r"\d+\.\s", line):
                line += "."
        fixed_lines.append(line)

    return "\n".join(fixed_lines).strip()

# LLM Ã§aÄŸÄ±rÄ±cÄ±
def invoke(data: dict):
    if "predict_result" not in data:
        try:
            with open("C:\\Users\\sozcu\\Desktop\\sample.json", "r") as f:
                measurements = json.load(f)
            voltage = sum([row["voltage"] for row in measurements]) / len(measurements)
            current = sum([row["current"] for row in measurements]) / len(measurements)
            active_power = sum([row["active_power"] for row in measurements]) / len(measurements)
            data["predict_result"] = predict_energy(voltage, current, active_power, n_days=30)
        except Exception as e:
            print(f"âš ï¸ Tahmin verisi yÃ¼klenemedi: {e}")
            data["predict_result"] = {}

    if "faults" not in data:
        try:
            faults = fault_detection("C:\\Users\\sozcu\\Desktop\\sample.json")
            data["faults"] = faults
            print("âœ… fault_detection Ã§alÄ±ÅŸtÄ±, sonuÃ§:", faults)
        except Exception as e:
            print(f"âš ï¸ ArÄ±za verisi yÃ¼klenemedi: {e}")
            data["faults"] = {}

    final_prompt = build_prompt(data)
    print("ğŸ“¢ Final Prompt:\n", final_prompt)  # prompt iÃ§eriÄŸini gÃ¶rmek iÃ§in

    raw_output = llm.invoke(final_prompt)
    return {"output": fix_output(raw_output.content)}
